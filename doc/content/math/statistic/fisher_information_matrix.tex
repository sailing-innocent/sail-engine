
Suppose we have a model parameterized by parameter vector $\theta$ that models a distribution $p(x|\theta)$. In freqentist statistics, the way we learn $\theta$ is to maximize the likelihood $p(x|\theta)$ wrt. parameter $\theta$

$s(\theta)=\nabla_{\theta}\log{p(x|\theta)}$

That is, score function is the gradient of log likelihood function.

The expected value of score wrt. our model is zero:

\begin{equation}
    \begin{aligned}
        \mathop{\mathbb{E}}\limits_{p(x|\theta)}[s(\theta)] 
        & = \mathop{\mathbb{E}}\limits_{p(x|\theta)}[\nabla{\log{p(x|\theta)}}] \\
        & = \int{\nabla{\log{p(x|\theta)}p(x|\theta)dx}} \\
        & = \int{\frac{\nabla p(x|\theta)}{p(x|\theta)}p(x|\theta)dx} \\ 
        & = \int{\nabla p(x|\theta) dx} \\ 
        & = \nabla 1 = 0
    \end{aligned}
\end{equation}

But how certain are we to our estimate? we can define an uncertainty measure around the expected estimates

$\mathop{\mathbb{E}}\limits_{p(x|\theta)}[ (s(\theta) - 0)(s(\theta) - 0)^T ]$

The convariance of score function above is the definition of Fisher Information. As we assume the Fisher Information is in a matrix form, called Fisher Information Matrix:

$F = \mathop{\mathbb{E}}\limits_{p(x|\theta)}[ \nabla\log p(x_i|\theta) \nabla\log (x_i|\theta)^T ]$

However, usually our likelihood si complicated and computing the expectation is intractable, we can approximate the expectation of F using empirical distribution $\hat{q}(x)$, which is given by our training data $X=\{x_1, x_2, \dots, x_N\}$.

In this form, Fisher information matrix is called Empirical Fisher:

$F=\frac{1}{N}\sum\limits_{i=1}\limits^{N}\nabla p(x_i|\theta)\nabla \log{p(x_i|\theta)^T}$

We may find that the Fisher Information Matrix is the interpretation of negative expected Hessian of our model's log likelihood.

The negative expected Hessian of log likelihood is equal to the Fisher Information Matrix F:

Supppose the gradient of log likelihood funciton : $g(x|\theta)=\frac{\nabla p(x|\theta)}{p(x|\theta)}$

[[Hessian|notes.math.elementary.hessian]] is given by the [[Jacobian|notes.math.elementary.jacobian]] of its gradient:

$H_{\log p(x|\theta)}=J(g(x|\theta))$

given quotient rule of derivative $\frac{\partial (\frac{\nabla_{\theta}p}{p})_j}{\partial x_i}=\frac{\frac{\partial (\nabla_\theta p)_j}{\partial x_i} - \frac{\partial p}{\partial x_i}(\nabla_\theta p)_j}{p^2}$

So $J(\frac{\nabla_\theta p}{p})=\frac{J(\nabla_\theta p)p-(\nabla_\theta p)(\nabla_\theta p)^T}{p(x|\theta)^2}$

Thus $H_{\log p(x|\theta)}=\frac{H_{p(x|\theta)}p(x|\theta)}{p(x|\theta)p(x|\theta)}-\frac{(\nabla_\theta p)(\nabla_\theta p)^T}{p(x|\theta)p(x|\theta)}=\frac{H_{p(x|\theta)}}{p(x|\theta)}-(\frac{\nabla_\theta p}{p(x|\theta)})(\frac{\nabla_\theta p}{p(x|\theta)})^T$

where we have 

\begin{equation}
    \begin{aligned}
    \mathop{\mathbb{E}}\limits_{p(x|\theta)}[ H_{\log{p(x|\theta)}}]
    & = \mathop{\mathbb{E}}\limits_{p(x|\theta)}[ \frac{H_{p(x|\theta)}}{p(x|\theta)} ] - \mathop{\mathbb{E}}\limits_{p(x|\theta)}[ (\frac{\nabla_\theta p}{p(x|\theta)})(\frac{\nabla_\theta p}{p(x|\theta)})^T ] \\
    & = \int \frac{H_{p(x|\theta)}}{p(x|\theta)} p(x|\theta)dx - \mathop{\mathbb{E}}\limits_{p(x|\theta)}[ \nabla_\theta log(p(x|\theta)) \nabla_\theta log(p(x|\theta))^T] \\ 
    & = H_{\int p(x|\theta)dx} - F \\ 
    & = H_1 - F = -F
    \end{aligned}
\end{equation}

Thus we have 

$F = -\mathop{\mathbb{E}}\limits_{p(x|\theta)}[ H_{\log{p(x|\theta)}} ]$