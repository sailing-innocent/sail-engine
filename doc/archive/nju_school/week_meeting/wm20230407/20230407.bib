@misc{yangDiffusionModelsComprehensive2023,
  title         = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle    = {Diffusion {{Models}}},
  author        = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2209.00796},
  eprint        = {arXiv:2209.00796},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2209.00796},
  urldate       = {2023-04-03},
  abstract      = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\8PSDSICM\\Yang ç­‰ - 2023 - Diffusion Models A Comprehensive Survey of Method.pdf;C\:\\Users\\Color\\Zotero\\storage\\V3TIHPCG\\2209.html}
}

@article{zhengLuisaRenderHighPerformanceRendering2022,
  title      = {{{LuisaRender}}: {{A High-Performance Rendering Framework}} with {{Layered}} and {{Unified Interfaces}} on {{Stream Architectures}}},
  shorttitle = {{{LuisaRender}}},
  author     = {Zheng, Shaokun and Zhou, Zhiqian and Chen, Xin and Yan, Difei and Zhang, Chuyan and Geng, Yuefeng and Gu, Yan and Xu, Kun},
  year       = {2022},
  month      = nov,
  journal    = {ACM Transactions on Graphics},
  volume     = {41},
  number     = {6},
  pages      = {232:1--232:19},
  issn       = {0730-0301},
  doi        = {10.1145/3550454.3555463},
  urldate    = {2023-04-07},
  abstract   = {The advancements in hardware have drawn more attention than ever to high-quality offline rendering with modern stream processors, both in the industry and in research fields. However, the graphics APIs are fragmented and existing shading languages lack high-level constructs such as polymorphism, which adds complexity to developing and maintaining cross-platform high-performance renderers. We present LuisaRender1, a high-performance rendering framework for modern stream-architecture hardware. Our main contribution is an expressive C++-embedded DSL for kernel programming with JIT code generation and compilation. We also implement a unified runtime layer with resource wrappers and an optimized Monte Carlo renderer. Experiments on test scenes show that LuisaRender achieves much higher performance than existing research renderers on modern graphics hardware, e.g., 5--11\texttimes{} faster than PBRT-v4 and 4--16\texttimes{} faster than Mitsuba 3.},
  keywords   = {cross-platform renderer,rendering framework,stream architecture},
  file       = {C\:\\Users\\Color\\Zotero\\storage\\65RVEBZ8\\Zheng et al. - 2022 - LuisaRender A High-Performance Rendering Framewor.pdf}
}
