@online{abou-chakraParticleNeRFParticleBasedEncoding2022,
  title = {{{ParticleNeRF}}: {{A Particle-Based Encoding}} for {{Online Neural Radiance Fields}}},
  shorttitle = {{{ParticleNeRF}}},
  author = {Abou-Chakra, Jad and Dayoub, Feras and Sünderhauf, Niko},
  date = {2022-11-08},
  url = {https://arxiv.org/abs/2211.04041v4},
  urldate = {2023-09-17},
  abstract = {While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline methods with an emphasis on visual fidelity, our paper addresses the online use case that prioritises real-time adaptability. We present ParticleNeRF, a new approach that dynamically adapts to changes in the scene geometry by learning an up-to-date representation online, every 200ms. ParticleNeRF achieves this using a novel particle-based parametric encoding. We couple features to particles in space and backpropagate the photometric reconstruction loss into the particles' position gradients, which are then interpreted as velocity vectors. Governed by a lightweight physics system to handle collisions, this lets the features move freely with the changing scene geometry. We demonstrate ParticleNeRF on various dynamic scenes containing translating, rotating, articulated, and deformable objects. ParticleNeRF is the first online dynamic NeRF and achieves fast adaptability with better visual fidelity than brute-force online InstantNGP and other baseline approaches on dynamic scenes with online constraints. Videos of our system can be found at our project website https://sites.google.com/view/particlenerf.},
  langid = {english},
  organization = {arXiv.org},
  file = {D:\data\zotero_storage\storage\BU7L3CNT\Abou-Chakra et al. - 2022 - ParticleNeRF A Particle-Based Encoding for Online.pdf}
}

@online{barronMipNeRF360Unbounded2022,
  title = {Mip-{{NeRF}} 360: {{Unbounded Anti-Aliased Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}} 360},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
  date = {2022-03-25},
  eprint = {2111.12077},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.12077},
  urldate = {2023-04-20},
  abstract = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on "unbounded" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57\% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\PZAMUU26\\Barron 等 - 2022 - Mip-NeRF 360 Unbounded Anti-Aliased Neural Radian.pdf;D\:\\data\\zotero_storage\\storage\\74HMMP62\\2111.html}
}

@online{barronMipNeRFMultiscaleRepresentation2021,
  title = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}}},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
  date = {2021-08-13},
  eprint = {2103.13415},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.13415},
  url = {http://arxiv.org/abs/2103.13415},
  urldate = {2023-04-20},
  abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\DJ2UITVV\\Barron 等 - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf;D\:\\data\\zotero_storage\\storage\\3IG3962K\\2103.html}
}

@online{barronZipNeRFAntiAliasedGridBased2023,
  title = {Zip-{{NeRF}}: {{Anti-Aliased Grid-Based Neural Radiance Fields}}},
  shorttitle = {Zip-{{NeRF}}},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
  date = {2023-05-21},
  eprint = {2304.06706},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06706},
  url = {http://arxiv.org/abs/2304.06706},
  urldate = {2023-07-04},
  abstract = {Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8\% - 77\% lower than either prior technique, and that trains 24x faster than mip-NeRF 360.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\552STNIV\\Barron 等 - 2023 - Zip-NeRF Anti-Aliased Grid-Based Neural Radiance .pdf;D\:\\data\\zotero_storage\\storage\\CBG733RH\\2304.html}
}

@article{bergmanGenerativeNeuralArticulated2022,
  title = {Generative {{Neural Articulated Radiance Fields}}},
  author = {Bergman, Alexander W. and Kellnhofer, Petr and Wang, Yifan and Chan, Eric and Lindell, David B. and Wetzstein, Gordon},
  date = {2022},
  journaltitle = {ArXiv},
  doi = {10.48550/arXiv.2206.14314},
  abstract = {This work develops a 3D GAN framework that learns to generate radiance of human bodies or faces in a canonical pose and warp them using an explicit deformation into a desired body pose or facial expression and demonstrates the first high-quality radiance generation results for human bodies. Unsupervised learning of 3D-aware generative adversarial networks (GANs) using only collections of single-view 2D photographs has very recently made much progress. These 3D GANs, however, have not been demonstrated for human bodies and the generated radiance fields of existing frameworks are not directly editable, limiting their applicability in downstream tasks. We propose a solution to these challenges by developing a 3D GAN framework that learns to generate radiance fields of human bodies or faces in a canonical pose and warp them using an explicit deformation field into a desired body pose or facial expression. Using our framework, we demonstrate the first high-quality radiance field generation results for human bodies. Moreover, we show that our deformation-aware training procedure significantly improves the quality of generated bodies or faces when editing their poses or facial expressions compared to a 3D GAN that is not trained with explicit deformations.},
  file = {D:\data\zotero_storage\storage\ZVG4TDW4\Bergman et al. - 2022 - Generative Neural Articulated Radiance Fields.pdf}
}

@online{bianNoPeNeRFOptimisingNeural2023,
  title = {{{NoPe-NeRF}}: {{Optimising Neural Radiance Field}} with {{No Pose Prior}}},
  shorttitle = {{{NoPe-NeRF}}},
  author = {Bian, Wenjing and Wang, Zirui and Li, Kejie and Bian, Jia-Wang and Prisacariu, Victor Adrian},
  date = {2023-04-14},
  eprint = {2212.07388},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.07388},
  url = {http://arxiv.org/abs/2212.07388},
  urldate = {2023-10-07},
  abstract = {Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy. Our project page is https://nope-nerf.active.vision.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\4643S8AP\\Bian 等 - 2023 - NoPe-NeRF Optimising Neural Radiance Field with N.pdf;D\:\\data\\zotero_storage\\storage\\T4MKFPGA\\2212.html}
}

@online{chenMobileNeRFExploitingPolygon2023,
  title = {{{MobileNeRF}}: {{Exploiting}} the {{Polygon Rasterization Pipeline}} for {{Efficient Neural Field Rendering}} on {{Mobile Architectures}}},
  shorttitle = {{{MobileNeRF}}},
  author = {Chen, Zhiqin and Funkhouser, Thomas and Hedman, Peter and Tagliasacchi, Andrea},
  date = {2023-05-29},
  eprint = {2208.00277},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2208.00277},
  urldate = {2023-09-28},
  abstract = {Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile phones.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D:\data\zotero_storage\storage\CG5VLHL6\Chen et al. - 2023 - MobileNeRF Exploiting the Polygon Rasterization P.pdf}
}

@online{chenStructureAwareNeRFPosed2022,
  title = {Structure-{{Aware NeRF}} without {{Posed Camera}} via {{Epipolar Constraint}}},
  author = {Chen, Shu and Zhang, Yang and Xu, Yaxin and Zou, Beiji},
  date = {2022-09-30},
  eprint = {2210.00183},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.00183},
  url = {http://arxiv.org/abs/2210.00183},
  urldate = {2023-04-19},
  abstract = {The neural radiance field (NeRF) for realistic novel view synthesis requires camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This two-stage strategy is not convenient to use and degrades the performance because the error in the pose extraction can propagate to the view synthesis. We integrate the pose extraction and view synthesis into a single end-to-end procedure so they can benefit from each other. For training NeRF models, only RGB images are given, without pre-known camera poses. The camera poses are obtained by the epipolar constraint in which the identical feature in different views has the same world coordinates transformed from the local camera coordinates according to the extracted poses. The epipolar constraint is jointly optimized with pixel color constraint. The poses are represented by a CNN-based deep network, whose input is the related frames. This joint optimization enables NeRF to be aware of the scene's structure that has an improved generalization performance. Extensive experiments on a variety of scenes demonstrate the effectiveness of the proposed approach. Code is available at https://github.com/XTU-PR-LAB/SaNerf.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\NAWUVCPE\\Chen 等 - 2022 - Structure-Aware NeRF without Posed Camera via Epip.pdf;D\:\\data\\zotero_storage\\storage\\SKH45LQL\\2210.html}
}

@online{chenTensoRFTensorialRadiance2022,
  title = {{{TensoRF}}: {{Tensorial Radiance Fields}}},
  shorttitle = {{{TensoRF}}},
  author = {Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao},
  date = {2022-11-29},
  eprint = {2203.09517},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.09517},
  urldate = {2024-04-10},
  abstract = {We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction ({$<$}30 min) with better rendering quality and even a smaller model size ({$<$}4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time ({$<$}10 min) and retaining a compact model size ({$<$}75 MB).},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\IS8TUK33\\Chen 等 - 2022 - TensoRF Tensorial Radiance Fields.pdf;D\:\\data\\zotero_storage\\storage\\IMJC9ETG\\2203.html}
}

@online{dengDepthsupervisedNeRFFewer2022,
  title = {Depth-Supervised {{NeRF}}: {{Fewer Views}} and {{Faster Training}} for {{Free}}},
  shorttitle = {Depth-Supervised {{NeRF}}},
  author = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
  date = {2022-04-29},
  eprint = {2107.02791},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2107.02791},
  url = {http://arxiv.org/abs/2107.02791},
  urldate = {2023-06-13},
  abstract = {A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as "free" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\KX9PGA6T\\Deng 等 - 2022 - Depth-supervised NeRF Fewer Views and Faster Trai.pdf;D\:\\data\\zotero_storage\\storage\\M2YFZIJU\\2107.html}
}

@inproceedings{gortlerLumigraph1996,
  title = {The Lumigraph},
  booktitle = {Proceedings of the 23rd Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
  date = {1996-08-01},
  series = {{{SIGGRAPH}} '96},
  pages = {43--54},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/237170.237200},
  url = {https://dl.acm.org/doi/10.1145/237170.237200},
  urldate = {2023-04-12},
  isbn = {978-0-89791-746-9},
  file = {D:\data\zotero_storage\storage\J9XE8HJJ\Gortler 等 - 1996 - The lumigraph.pdf}
}

@online{guNerfDiffSingleimageView2023,
  title = {{{NerfDiff}}: {{Single-image View Synthesis}} with {{NeRF-guided Distillation}} from {{3D-aware Diffusion}}},
  shorttitle = {{{NerfDiff}}},
  author = {Gu, Jiatao and Trevithick, Alex and Lin, Kai-En and Susskind, Josh and Theobalt, Christian and Liu, Lingjie and Ramamoorthi, Ravi},
  date = {2023-02-20},
  eprint = {2302.10109},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.10109},
  url = {http://arxiv.org/abs/2302.10109},
  urldate = {2023-04-11},
  abstract = {Novel view synthesis from a single image requires inferring occluded regions of objects and scenes whilst simultaneously maintaining semantic and physical consistency with the input. Existing approaches condition neural radiance fields (NeRF) on local image features, projecting points to the input image plane, and aggregating 2D features to perform volume rendering. However, under severe occlusion, this projection fails to resolve uncertainty, resulting in blurry renderings that lack details. In this work, we propose NerfDiff, which addresses this issue by distilling the knowledge of a 3D-aware conditional diffusion model (CDM) into NeRF through synthesizing and refining a set of virtual views at test time. We further propose a novel NeRF-guided distillation algorithm that simultaneously generates 3D consistent virtual views from the CDM samples, and finetunes the NeRF based on the improved virtual views. Our approach significantly outperforms existing NeRF-based and geometry-free approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\VZ7B98VR\\Gu 等 - 2023 - NerfDiff Single-image View Synthesis with NeRF-gu.pdf;D\:\\data\\zotero_storage\\storage\\G5J4XH84\\2302.html}
}

@online{huHighfidelity3DReconstruction2023,
  title = {High-Fidelity {{3D Reconstruction}} of {{Plants}} Using {{Neural Radiance Field}}},
  author = {Hu, Kewei and Wei, Ying and Pan, Yaoqiang and Kang, Hanwen and Chen, Chao},
  date = {2023-11-07},
  eprint = {2311.04154},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.04154},
  urldate = {2023-11-08},
  abstract = {Accurate reconstruction of plant phenotypes plays a key role in optimising sustainable farming practices in the field of Precision Agriculture (PA). Currently, optical sensor-based approaches dominate the field, but the need for high-fidelity 3D reconstruction of crops and plants in unstructured agricultural environments remains challenging. Recently, a promising development has emerged in the form of Neural Radiance Field (NeRF), a novel method that utilises neural density fields. This technique has shown impressive performance in various novel vision synthesis tasks, but has remained relatively unexplored in the agricultural context. In our study, we focus on two fundamental tasks within plant phenotyping: (1) the synthesis of 2D novel-view images and (2) the 3D reconstruction of crop and plant models. We explore the world of neural radiance fields, in particular two SOTA methods: Instant-NGP, which excels in generating high-quality images with impressive training and inference speed, and Instant-NSR, which improves the reconstructed geometry by incorporating the Signed Distance Function (SDF) during training. In particular, we present a novel plant phenotype dataset comprising real plant images from production environments. This dataset is a first-of-its-kind initiative aimed at comprehensively exploring the advantages and limitations of NeRF in agricultural contexts. Our experimental results show that NeRF demonstrates commendable performance in the synthesis of novel-view images and is able to achieve reconstruction results that are competitive with Reality Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based reconstruction. However, our study also highlights certain drawbacks of NeRF, including relatively slow training speeds, performance limitations in cases of insufficient sampling, and challenges in obtaining geometry quality in complex setups.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {D\:\\data\\zotero_storage\\storage\\5RFQGZTF\\Hu 等 - 2023 - High-fidelity 3D Reconstruction of Plants using Ne.pdf;D\:\\data\\zotero_storage\\storage\\IMLA6CII\\2311.html}
}

@online{jun-seongHDRPlenoxelsSelfCalibratingHigh2022,
  title = {{{HDR-Plenoxels}}: {{Self-Calibrating High Dynamic Range Radiance Fields}}},
  shorttitle = {{{HDR-Plenoxels}}},
  author = {Jun-Seong, Kim and Yu-Ji, Kim and Ye-Bin, Moon and Oh, Tae-Hyun},
  date = {2022-11-18},
  eprint = {2208.06787},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.06787},
  url = {http://arxiv.org/abs/2208.06787},
  urldate = {2024-01-29},
  abstract = {We propose high dynamic range (HDR) radiance fields, HDR-Plenoxels, that learn a plenoptic function of 3D HDR radiance fields, geometry information, and varying camera settings inherent in 2D low dynamic range (LDR) images. Our voxel-based volume rendering pipeline reconstructs HDR radiance fields with only multi-view LDR images taken from varying camera settings in an end-to-end manner and has a fast convergence speed. To deal with various cameras in real-world scenarios, we introduce a tone mapping module that models the digital in-camera imaging pipeline (ISP) and disentangles radiometric settings. Our tone mapping module allows us to render by controlling the radiometric settings of each novel view. Finally, we build a multi-view dataset with varying camera conditions, which fits our problem setting. Our experiments show that HDR-Plenoxels can express detail and high-quality HDR novel views from only LDR images with various cameras.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\SYI2WTUC\\Jun-Seong 等 - 2022 - HDR-Plenoxels Self-Calibrating High Dynamic Range.pdf;D\:\\data\\zotero_storage\\storage\\44PAFSJ2\\2208.html}
}

@article{kimAENeRFAutoEncodingNeural2022,
  title = {{{AE-NeRF}}: {{Auto-Encoding Neural Radiance Fields}} for {{3D-Aware Object Manipulation}}},
  shorttitle = {{{AE-NeRF}}},
  author = {Kim, Mira and Ko, Jae-Sub and Cho, Kyusun and Choi, J. and Choi, Daewon and Kim, Seung Wook},
  date = {2022},
  journaltitle = {ArXiv},
  doi = {10.48550/arXiv.2204.13426},
  abstract = {A novel framework for 3D-aware object manipulation, called Auto-Encoding Neural Radiance Fields (AE-NeRF), which is formulated in an auto-encoder architecture that extracts disentangled 3D attributes such as 3D shape, appearance, and camera pose from an image, and a high-quality image is rendered from the attributes through disentangling generative Neural Radiances Fields (NeRF). —We propose a novel framework for 3D-aware object manipulation, called Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D shape, appearance, and camera pose from an image, and a high-quality image is rendered from the attributes through disentangled generative Neural Radiance Fields (NeRF). To improve the disentanglement ability, we present two losses, global-local attribute consistency loss defined between input and output, and swapped-attribute classification loss. Since training such auto-encoding networks from scratch without ground-truth shape and appearance information is non-trivial, we present a stage-wise training scheme, which dramatically helps to boost the performance. We conduct experiments to demonstrate the effectiveness of the proposed model over the latest methods and provide extensive ablation studies.},
  file = {D:\data\zotero_storage\storage\5H4HRDBF\Kim et al. - 2022 - AE-NeRF Auto-Encoding Neural Radiance Fields for .pdf}
}

@article{kobayashiDecomposingNeRFEditing2022,
  title = {Decomposing {{NeRF}} for {{Editing}} via {{Feature Field Distillation}}},
  author = {Kobayashi, Sosuke and Matsumoto, Eiichi and Sitzmann, V.},
  date = {2022},
  journaltitle = {ArXiv},
  doi = {10.48550/arXiv.2205.15585},
  abstract = {This work tackles the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes, and distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors into a 3D feature field optimized in parallel to the radiance field. Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, supervised and self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations.},
  file = {D:\data\zotero_storage\storage\VN2C72BD\Kobayashi et al. - 2022 - Decomposing NeRF for Editing via Feature Field Dis.pdf}
}

@online{KPlanesExplicitRadiance2023,
  title = {K-{{Planes}}: {{Explicit Radiance Fields}} in {{Space}}, {{Time}}, and {{Appearance}}},
  shorttitle = {K-{{Planes}}},
  author = {Fridovich-Keil, Sara and Meanti, Giacomo and Warburg, Frederik and Recht, Benjamin and Kanazawa, Angjoo},
  date = {2023-03-24},
  eprint = {2301.10241},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.10241},
  urldate = {2023-10-18},
  abstract = {We introduce k-planes, a white-box model for radiance fields in arbitrary dimensions. Our model uses d choose 2 planes to represent a d-dimensional scene, providing a seamless way to go from static (d=3) to dynamic (d=4) scenes. This planar factorization makes adding dimension-specific priors easy, e.g. temporal smoothness and multi-resolution spatial structure, and induces a natural decomposition of static and dynamic components of a scene. We use a linear feature decoder with a learned color basis that yields similar performance as a nonlinear black-box MLP decoder. Across a range of synthetic and real, static and dynamic, fixed and varying appearance scenes, k-planes yields competitive and often state-of-the-art reconstruction fidelity with low memory usage, achieving 1000x compression over a full 4D grid, and fast optimization with a pure PyTorch implementation. For video results and code, please see https://sarafridov.github.io/K-Planes.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\L62CD8NA\\Fridovich-Keil 等 - 2023 - K-Planes Explicit Radiance Fields in Space, Time,.pdf;D\:\\data\\zotero_storage\\storage\\K74L3625\\2301.html}
}

@online{kulhanekTetraNeRFRepresentingNeural2023,
  title = {Tetra-{{NeRF}}: {{Representing Neural Radiance Fields Using Tetrahedra}}},
  shorttitle = {Tetra-{{NeRF}}},
  author = {Kulhanek, Jonas and Sattler, Torsten},
  date = {2023-04-19},
  eprint = {2304.09987},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.09987},
  url = {http://arxiv.org/abs/2304.09987},
  urldate = {2023-04-25},
  abstract = {Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\XS9FPK44\\Kulhanek 和 Sattler - 2023 - Tetra-NeRF Representing Neural Radiance Fields Us.pdf;D\:\\data\\zotero_storage\\storage\\T6IHYTQS\\2304.html}
}

@online{leeDPNeRFDeblurredNeural2023,
  title = {{{DP-NeRF}}: {{Deblurred Neural Radiance Field}} with {{Physical Scene Priors}}},
  shorttitle = {{{DP-NeRF}}},
  author = {Lee, Dogyoon and Lee, Minhyeok and Shin, Chajin and Lee, Sangyoun},
  date = {2023-03-08},
  eprint = {2211.12046},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.12046},
  urldate = {2023-04-17},
  abstract = {Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D) reconstruction quality via the novel view synthesis from multi-view images and paired calibrated camera parameters. However, previous NeRF-based systems have been demonstrated under strictly controlled settings, with little attention paid to less ideal scenarios, including with the presence of noise such as exposure, illumination changes, and blur. In particular, though blur frequently occurs in real situations, NeRF that can handle blurred images has received little attention. The few studies that have investigated NeRF for blurred images have not considered geometric and appearance consistency in 3D space, which is one of the most important factors in 3D reconstruction. This leads to inconsistency and the degradation of the perceptual quality of the constructed scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for blurred images, which is constrained with two physical priors. These priors are derived from the actual blurring process during image acquisition by the camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency utilizing the physical priors and adaptive weight proposal to refine the color composition error in consideration of the relationship between depth and blur. We present extensive experimental results for synthetic and real scenes with two types of blur: camera motion blur and defocus blur. The results demonstrate that DP-NeRF successfully improves the perceptual quality of the constructed NeRF ensuring 3D geometric and appearance consistency. We further demonstrate the effectiveness of our model with comprehensive ablation analysis.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\9B34DDDN\\Lee 等 - 2023 - DP-NeRF Deblurred Neural Radiance Field with Phys.pdf;D\:\\data\\zotero_storage\\storage\\4EZRPAY4\\2211.html}
}

@online{liLift3DSynthesize3D2023,
  title = {{{Lift3D}}: {{Synthesize 3D Training Data}} by {{Lifting 2D GAN}} to {{3D Generative Radiance Field}}},
  shorttitle = {{{Lift3D}}},
  author = {Li, Leheng and Lian, Qing and Wang, Luozhou and Ma, Ningning and Chen, Ying-Cong},
  date = {2023-04-07},
  eprint = {2304.03526},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.03526},
  urldate = {2023-04-27},
  abstract = {This work explores the use of 3D generative models to synthesize training data for 3D vision tasks. The key requirements of the generative models are that the generated data should be photorealistic to match the real-world scenarios, and the corresponding 3D attributes should be aligned with given sampling labels. However, we find that the recent NeRF-based 3D GANs hardly meet the above requirements due to their designed generation pipeline and the lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted 2D-to-3D generation framework to achieve the data generation objectives. Lift3D has several merits compared to prior methods: (1) Unlike previous 3D GANs that the output resolution is fixed after training, Lift3D can generalize to any camera intrinsic with higher resolution and photorealistic output. (2) By lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D information of generated objects, thus offering accurate 3D annotations for downstream tasks. We evaluate the effectiveness of our framework by augmenting autonomous driving datasets. Experimental results demonstrate that our data generation framework can effectively improve the performance of 3D object detectors. Project page: https://len-li.github.io/lift3d-web.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\AWAAMHKI\\Li 等 - 2023 - Lift3D Synthesize 3D Training Data by Lifting 2D .pdf;D\:\\data\\zotero_storage\\storage\\8M3N294G\\2304.html}
}

@inproceedings{liPACNeRFPhysicsAugmented2023,
  title = {{{PAC-NeRF}}: {{Physics Augmented Continuum Neural Radiance Fields}} for {{Geometry-Agnostic System Identification}}},
  shorttitle = {{{PAC-NeRF}}},
  author = {Li, Xuan and Qiao, Yi-Ling and Chen, Peter Yichen and Jatavallabhula, Krishna Murthy and Lin, Ming and Jiang, Chenfanfu and Gan, Chuang},
  date = {2023-02-01},
  url = {https://openreview.net/forum?id=tVkrbkz42vc},
  urldate = {2023-04-17},
  abstract = {Existing approaches to system identification (estimating the physical parameters of an object) from videos assume known object geometries. This precludes their applicability in a vast majority of scenes where object geometries are complex or unknown. In this work, we aim to identify parameters characterizing a physical system from a set of multi-view videos without any assumption on object geometry or topology. To this end, we propose "Physics Augmented Continuum Neural Radiance Fields" (PAC-NeRF), to estimate both the unknown geometry and physical parameters of highly dynamic objects from multi-view videos. We design PAC-NeRF to only ever produce physically plausible states by enforcing the neural radiance field to follow the conservation laws of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian representation of the neural radiance field, i.e., we use the Eulerian grid representation for NeRF density and color fields, while advecting the neural radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian representation seamlessly blends efficient neural rendering with the material point method (MPM) for robust differentiable physics simulation. We validate the effectiveness of our proposed framework on geometry and physical parameter estimation over a vast range of materials, including elastic bodies, plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate significant performance gain on most tasks.},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {D:\data\zotero_storage\storage\QUXTVPDX\Li 等 - 2023 - PAC-NeRF Physics Augmented Continuum Neural Radia.pdf}
}

@online{liuNeuralRaysOcclusionaware2022,
  title = {Neural {{Rays}} for {{Occlusion-aware Image-based Rendering}}},
  author = {Liu, Yuan and Peng, Sida and Liu, Lingjie and Wang, Qianqian and Wang, Peng and Theobalt, Christian and Zhou, Xiaowei and Wang, Wenping},
  date = {2022-03-28},
  eprint = {2107.13421},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2107.13421},
  url = {http://arxiv.org/abs/2107.13421},
  urldate = {2022-12-28},
  abstract = {We present a new neural representation, called Neural Ray (NeuRay), for the novel view synthesis task. Recent works construct radiance fields from image features of input views to render novel view images, which enables the generalization to new scenes. However, due to occlusions, a 3D point may be invisible to some input views. On such a 3D point, these generalization methods will include inconsistent image features from invisible views, which interfere with the radiance field construction. To solve this problem, we predict the visibility of 3D points to input views within our NeuRay representation. This visibility enables the radiance field construction to focus on visible image features, which significantly improves its rendering quality. Meanwhile, a novel consistency loss is proposed to refine the visibility in NeuRay when finetuning on a specific scene. Experiments demonstrate that our approach achieves state-of-the-art performance on the novel view synthesis task when generalizing to unseen scenes and outperforms per-scene optimization methods after finetuning.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\CRSIKDEN\\Liu 等 - 2022 - Neural Rays for Occlusion-aware Image-based Render.pdf;D\:\\data\\zotero_storage\\storage\\EIH4DGTA\\2107.html}
}

@online{luPanoNeRFSynthesizingHigh2023,
  title = {Pano-{{NeRF}}: {{Synthesizing High Dynamic Range Novel Views}} with {{Geometry}} from {{Sparse Low Dynamic Range Panoramic Images}}},
  shorttitle = {Pano-{{NeRF}}},
  author = {Lu, Zhan and Zheng, Qian and Shi, Boxin and Jiang, Xudong},
  date = {2023-12-26},
  eprint = {2312.15942},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2312.15942},
  url = {http://arxiv.org/abs/2312.15942},
  urldate = {2024-01-23},
  abstract = {Panoramic imaging research on geometry recovery and High Dynamic Range (HDR) reconstruction becomes a trend with the development of Extended Reality (XR). Neural Radiance Fields (NeRF) provide a promising scene representation for both tasks without requiring extensive prior data. However, in the case of inputting sparse Low Dynamic Range (LDR) panoramic images, NeRF often degrades with under-constrained geometry and is unable to reconstruct HDR radiance from LDR inputs. We observe that the radiance from each pixel in panoramic images can be modeled as both a signal to convey scene lighting information and a light source to illuminate other pixels. Hence, we propose the irradiance fields from sparse LDR panoramic images, which increases the observation counts for faithful geometry recovery and leverages the irradiance-radiance attenuation for HDR reconstruction. Extensive experiments demonstrate that the irradiance fields outperform state-of-the-art methods on both geometry recovery and HDR reconstruction and validate their effectiveness. Furthermore, we show a promising byproduct of spatially-varying lighting estimation. The code is available at https://github.com/Lu-Zhan/Pano-NeRF.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {D\:\\data\\zotero_storage\\storage\\4PLBNFRV\\Lu 等 - 2023 - Pano-NeRF Synthesizing High Dynamic Range Novel V.pdf;D\:\\data\\zotero_storage\\storage\\G7VWM5JL\\2312.html}
}

@online{martin-bruallaNeRFWildNeural2021,
  title = {{{NeRF}} in the {{Wild}}: {{Neural Radiance Fields}} for {{Unconstrained Photo Collections}}},
  shorttitle = {{{NeRF}} in the {{Wild}}},
  author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
  date = {2021-01-06},
  eprint = {2008.02268},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2008.02268},
  url = {http://arxiv.org/abs/2008.02268},
  urldate = {2023-04-27},
  abstract = {We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\Q5CKNP75\\Martin-Brualla 等 - 2021 - NeRF in the Wild Neural Radiance Fields for Uncon.pdf;D\:\\data\\zotero_storage\\storage\\6MW2VAFC\\2008.html}
}

@online{meulemanProgressivelyOptimizedLocal2023,
  title = {Progressively {{Optimized Local Radiance Fields}} for {{Robust View Synthesis}}},
  author = {Meuleman, Andreas and Liu, Yu-Lun and Gao, Chen and Huang, Jia-Bin and Kim, Changil and Kim, Min H. and Kopf, Johannes},
  date = {2023-03-24},
  eprint = {2303.13791},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.13791},
  url = {http://arxiv.org/abs/2303.13791},
  urldate = {2023-04-02},
  abstract = {We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\4VSMEBHN\\Meuleman et al. - 2023 - Progressively Optimized Local Radiance Fields for .pdf;D\:\\data\\zotero_storage\\storage\\HFZE2JJL\\2303.html}
}

@online{mildenhallLocalLightField2019,
  title = {Local {{Light Field Fusion}}: {{Practical View Synthesis}} with {{Prescriptive Sampling Guidelines}}},
  shorttitle = {Local {{Light Field Fusion}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Ortiz-Cayon, Rodrigo and Kalantari, Nima Khademi and Ramamoorthi, Ravi and Ng, Ren and Kar, Abhishek},
  date = {2019-05-02},
  eprint = {1905.00889},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.00889},
  url = {http://arxiv.org/abs/1905.00889},
  urldate = {2023-03-06},
  abstract = {We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\SDR2GUYE\\Mildenhall et al. - 2019 - Local Light Field Fusion Practical View Synthesis.pdf;D\:\\data\\zotero_storage\\storage\\N8DEHSVB\\1905.html}
}

@inproceedings{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  booktitle = {{{ECCV}}},
  author = {Mildenhall, B. and Srinivasan, P. and Tancik, Matthew and Barron, J. and Ramamoorthi, R. and Ng, Ren},
  date = {2020},
  doi = {10.1007/978-3-030-58452-8_24},
  abstract = {This work describes how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrates results that outperform prior work on neural rendering and view synthesis. We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  file = {D:\data\zotero_storage\storage\74B8SHTC\nerf_paper.pdf}
}

@article{mullerInstantNeuralGraphics2022,
  title = {Instant {{Neural Graphics Primitives}} with a {{Multiresolution Hash Encoding}}},
  author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  date = {2022-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  eprint = {2201.05989},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530127},
  url = {http://arxiv.org/abs/2201.05989},
  urldate = {2023-04-20},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of \$\{1920\textbackslash!\textbackslash times\textbackslash!1080\}\$.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\T4TWTCUV\\Müller 等 - 2022 - Instant Neural Graphics Primitives with a Multires.pdf;D\:\\data\\zotero_storage\\storage\\ERVZ6HRM\\2201.html}
}

@inproceedings{mullerInstantNeuralRadiance2022,
  title = {Instant {{Neural Radiance Fields}}},
  booktitle = {{{ACM SIGGRAPH}} 2022 {{Real-Time Live}}!},
  author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Foco, Marco and Bódis-Szomorú, András and Deutsch, Isaac and Shelley, Michael and Keller, Alexander},
  date = {2022-07-24},
  series = {{{SIGGRAPH}} '22},
  pages = {1--2},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3532833.3538678},
  url = {https://dl.acm.org/doi/10.1145/3532833.3538678},
  urldate = {2023-04-19},
  abstract = {We extend our instant NeRF implementation [Müller et al. 2022] to allow training from an incremental stream of images and camera poses, provided by a realtime Simultaneous Localization And Mapping (SLAM) system. Camera poses are refined end-to-end by back-propagating the gradients from NeRF training. Reconstruction quality is further improved by compensating for various camera properties, such as rolling shutter, non-linear lens distortion, and variable exposure typical of digital cameras. Static scenes can be scanned, the NeRF model trained, and the reconstruction verified in an interactive fashion, in under a minute.},
  isbn = {978-1-4503-9368-3},
  file = {D:\data\zotero_storage\storage\FWCSL78S\Müller 等 - 2022 - Instant Neural Radiance Fields.pdf}
}

@online{niemeyerGIRAFFERepresentingScenes2021,
  title = {{{GIRAFFE}}: {{Representing Scenes}} as {{Compositional Generative Neural Feature Fields}}},
  shorttitle = {{{GIRAFFE}}},
  author = {Niemeyer, Michael and Geiger, Andreas},
  date = {2021-04-29},
  eprint = {2011.12100},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.12100},
  url = {http://arxiv.org/abs/2011.12100},
  urldate = {2023-04-27},
  abstract = {Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\8YV9I63M\\Niemeyer 和 Geiger - 2021 - GIRAFFE Representing Scenes as Compositional Gene.pdf;D\:\\data\\zotero_storage\\storage\\DSTHVB9D\\2011.html}
}

@online{PDFHNeRFNeural,
  title = {[{{PDF}}] {{H-NeRF}}: {{Neural Radiance Fields}} for {{Rendering}} and {{Temporal Reconstruction}} of {{Humans}} in {{Motion}} | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/H-NeRF%3A-Neural-Radiance-Fields-for-Rendering-and-of-Xu-Alldieck/e67b57dec4da780b5cb4bcb33498a1a329afe53d},
  urldate = {2022-07-19}
}

@online{PDFNeRFAnalyzing,
  title = {[{{PDF}}] {{NeRF}}++: {{Analyzing}} and {{Improving Neural Radiance Fields}} | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/NeRF%2B%2B%3A-Analyzing-and-Improving-Neural-Radiance-Zhang-Riegler/5b0ea2c92ee16fa2f5a3dbc9315cd5c1e4ec1d88},
  urldate = {2022-07-19},
  file = {D:\data\zotero_storage\storage\TZFHC85A\5b0ea2c92ee16fa2f5a3dbc9315cd5c1e4ec1d88.html}
}

@online{PDFNeRFNeural,
  title = {[{{PDF}}] {{NeRF-}}: {{Neural Radiance Fields Without Known Camera Parameters}} | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/NeRF-%3A-Neural-Radiance-Fields-Without-Known-Camera-Wang-Wu/6034aa4be2520537e02b707a31abc0d033673c85},
  urldate = {2022-07-19},
  file = {D:\data\zotero_storage\storage\M56S2LSY\6034aa4be2520537e02b707a31abc0d033673c85.html}
}

@online{PDFPuttingNeRF,
  title = {[{{PDF}}] {{Putting NeRF}} on a {{Diet}}: {{Semantically Consistent Few-Shot View Synthesis}} | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/Putting-NeRF-on-a-Diet%3A-Semantically-Consistent-Jain-Tancik/81918488b569df3a43fd998d7b698fa9f6d1ff1b},
  urldate = {2022-07-19},
  file = {D:\data\zotero_storage\storage\G3VLZXSX\81918488b569df3a43fd998d7b698fa9f6d1ff1b.html}
}

@inproceedings{pengAnimatableImplicitNeural2022,
  title = {Animatable {{Implicit Neural Representations}} for {{Creating Realistic Avatars}} from {{Videos}}},
  author = {Peng, Sida and Xu, Zhenqi and Dong, Junting and Wang, Qianqian and Zhang, Shang-Wei and Shuai, Qing and Bao, H. and Zhou, Xiaowei},
  date = {2022},
  abstract = {A pose-driven deformation based on the linear blend skinning algorithm, which combines the blend weight and the 3D human skeleton to produce observation-to-canonical correspondences, which outperforms recent human modeling methods. —This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce a pose-driven deformation field based on the linear blend skinning algorithm, which combines the blend weight field and the 3D human skeleton to produce observation-to-canonical correspondences. Since 3D human skeletons are more observable, they can regularize the learning of the deformation field. Moreover, the pose-driven deformation field can be controlled by input skeletal motions to generate new deformation fields to animate the canonical human model. Experiments show that our approach significantly outperforms recent human modeling methods. The code is available at https://zju3dv.github.io/animatable nerf/.},
  file = {D:\data\zotero_storage\storage\8XFBJD25\Peng et al. - 2022 - Animatable Implicit Neural Representations for Cre.pdf}
}

@online{pooleDreamFusionTextto3DUsing2022,
  title = {{{DreamFusion}}: {{Text-to-3D}} Using {{2D Diffusion}}},
  shorttitle = {{{DreamFusion}}},
  author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  date = {2022-09-29},
  eprint = {2209.14988},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2209.14988},
  url = {http://arxiv.org/abs/2209.14988},
  urldate = {2023-01-10},
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\IBRAU8IH\\Poole 等 - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf;D\:\\data\\zotero_storage\\storage\\A4VRFYXF\\2209.html}
}

@inproceedings{pumarolaDNeRFNeuralRadiance2021,
  title = {D-{{NeRF}}: {{Neural Radiance Fields}} for {{Dynamic Scenes}}},
  shorttitle = {D-{{NeRF}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
  date = {2021-06},
  pages = {10313--10322},
  publisher = {IEEE},
  location = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01018},
  url = {https://ieeexplore.ieee.org/document/9578753/},
  urldate = {2023-06-15},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D:\data\zotero_storage\storage\CJD3LHL3\Pumarola 等 - 2021 - D-NeRF Neural Radiance Fields for Dynamic Scenes.pdf}
}

@online{reiserMERFMemoryEfficientRadiance2023,
  title = {{{MERF}}: {{Memory-Efficient Radiance Fields}} for {{Real-time View Synthesis}} in {{Unbounded Scenes}}},
  shorttitle = {{{MERF}}},
  author = {Reiser, Christian and Szeliski, Richard and Verbin, Dor and Srinivasan, Pratul P. and Mildenhall, Ben and Geiger, Andreas and Barron, Jonathan T. and Hedman, Peter},
  date = {2023-02-23},
  eprint = {2302.12249},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.12249},
  url = {http://arxiv.org/abs/2302.12249},
  urldate = {2023-04-11},
  abstract = {Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\BLHA28D9\\Reiser 等 - 2023 - MERF Memory-Efficient Radiance Fields for Real-ti.pdf;D\:\\data\\zotero_storage\\storage\\JWGY6X8M\\2302.html}
}

@online{sunDirectVoxelGrid2022,
  title = {Direct {{Voxel Grid Optimization}}: {{Super-fast Convergence}} for {{Radiance Fields Reconstruction}}},
  shorttitle = {Direct {{Voxel Grid Optimization}}},
  author = {Sun, Cheng and Sun, Min and Chen, Hwann-Tzong},
  date = {2022-06-03},
  eprint = {2111.11215},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.11215},
  url = {http://arxiv.org/abs/2111.11215},
  urldate = {2023-04-17},
  abstract = {We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality and flexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\W6VX8U39\\Sun 等 - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf;D\:\\data\\zotero_storage\\storage\\3NI3PFKN\\2111.html}
}

@online{wangAdaptiveShellsEfficient2023,
  title = {Adaptive {{Shells}} for {{Efficient Neural Radiance Field Rendering}}},
  author = {Wang, Zian and Shen, Tianchang and Nimier-David, Merlin and Sharp, Nicholas and Gao, Jun and Keller, Alexander and Fidler, Sanja and Müller, Thomas and Gojcic, Zan},
  date = {2023-11-16},
  eprint = {2311.10091},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.10091},
  urldate = {2023-12-12},
  abstract = {Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images. Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization. Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel. Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity. Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To this end, we generalize the NeuS formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band. At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Experiments show that our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\98KZSCLW\\Wang 等 - 2023 - Adaptive Shells for Efficient Neural Radiance Fiel.pdf;D\:\\data\\zotero_storage\\storage\\UZ8C7KK2\\2311.html}
}

@online{wangCLIPNeRFTextandImageDriven2022,
  title = {{{CLIP-NeRF}}: {{Text-and-Image Driven Manipulation}} of {{Neural Radiance Fields}}},
  shorttitle = {{{CLIP-NeRF}}},
  author = {Wang, Can and Chai, Menglei and He, Mingming and Chen, Dongdong and Liao, Jing},
  date = {2022-03-02},
  eprint = {2112.05139},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.05139},
  url = {http://arxiv.org/abs/2112.05139},
  urldate = {2023-04-27},
  abstract = {We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural radiance fields (NeRF). By leveraging the joint language-image embedding space of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose a unified framework that allows manipulating NeRF in a user-friendly way, using either a short text prompt or an exemplar image. Specifically, to combine the novel view synthesis capability of NeRF and the controllable manipulation ability of latent representations from generative models, we introduce a disentangled conditional NeRF architecture that allows individual control over both shape and appearance. This is achieved by performing the shape conditioning via applying a learned deformation field to the positional encoding and deferring color conditioning to the volumetric rendering stage. To bridge this disentangled latent representation to the CLIP embedding, we design two code mappers that take a CLIP embedding as input and update the latent codes to reflect the targeted editing. The mappers are trained with a CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we propose an inverse optimization method that accurately projects an input image to the latent codes for manipulation to enable editing on real images. We evaluate our approach by extensive experiments on a variety of text prompts and exemplar images and also provide an intuitive interface for interactive editing. Our implementation is available at https://cassiepython.github.io/clipnerf/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\DQVCDXS7\\Wang 等 - 2022 - CLIP-NeRF Text-and-Image Driven Manipulation of N.pdf;D\:\\data\\zotero_storage\\storage\\MQKR9LEL\\2112.html}
}

@article{wangImageQualityAssessment2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  date = {2004-04},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {Data mining,Degradation,Humans,Image quality,Indexes,Layout,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {D:\data\zotero_storage\storage\KQ73YMSJ\1284395.html}
}

@online{wangNeRFFastNeural2023,
  title = {F\$\^{}\{2\}\$-{{NeRF}}: {{Fast Neural Radiance Field Training}} with {{Free Camera Trajectories}}},
  shorttitle = {F\$\^{}\{2\}\$-{{NeRF}}},
  author = {Wang, Peng and Liu, Yuan and Chen, Zhaoxi and Liu, Lingjie and Liu, Ziwei and Komura, Taku and Theobalt, Christian and Wang, Wenping},
  date = {2023-03-28},
  eprint = {2303.15951},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.15951},
  url = {http://arxiv.org/abs/2303.15951},
  urldate = {2023-05-17},
  abstract = {This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF) for novel view synthesis, which enables arbitrary input camera trajectories and only costs a few minutes for training. Existing fast grid-based NeRF training frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed for bounded scenes and rely on space warping to handle unbounded scenes. Existing two widely-used space-warping methods are only designed for the forward-facing trajectory or the 360-degree object-centric trajectory but cannot process arbitrary trajectories. In this paper, we delve deep into the mechanism of space warping to handle unbounded scenes. Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework. Extensive experiments demonstrate that F2-NeRF is able to use the same perspective warping to render high-quality images on two standard datasets and a new free trajectory dataset collected by us. Project page: https://totoro97.github.io/projects/f2-nerf.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\I6PHHWVH\\Wang et al. - 2023 - F$^ 2 $-NeRF Fast Neural Radiance Field Training .pdf;D\:\\data\\zotero_storage\\storage\\W9U64GEV\\2303.html}
}

@online{wangPFLRMPoseFreeLarge2023,
  title = {{{PF-LRM}}: {{Pose-Free Large Reconstruction Model}} for {{Joint Pose}} and {{Shape Prediction}}},
  shorttitle = {{{PF-LRM}}},
  author = {Wang, Peng and Tan, Hao and Bi, Sai and Xu, Yinghao and Luan, Fujun and Sunkavalli, Kalyan and Wang, Wenping and Xu, Zexiang and Zhang, Kai},
  date = {2023-11-23},
  eprint = {2311.12024},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.12024},
  urldate = {2023-11-27},
  abstract = {We propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstructing a 3D object from a few unposed images even with little visual overlap, while simultaneously estimating the relative camera poses in \textasciitilde 1.3 seconds on a single A100 GPU. PF-LRM is a highly scalable method utilizing the self-attention blocks to exchange information between 3D object tokens and 2D image tokens; we predict a coarse point cloud for each view, and then use a differentiable Perspective-n-Point (PnP) solver to obtain camera poses. When trained on a huge amount of multi-view posed data of \textasciitilde 1M objects, PF-LRM shows strong cross-dataset generalization ability, and outperforms baseline methods by a large margin in terms of pose prediction accuracy and 3D reconstruction quality on various unseen evaluation datasets. We also demonstrate our model's applicability in downstream text/image-to-3D task with fast feed-forward inference. Our project website is at: https://totoro97.github.io/pf-lrm .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\GPAGW4DL\\Wang 等 - 2023 - PF-LRM Pose-Free Large Reconstruction Model for J.pdf;D\:\\data\\zotero_storage\\storage\\JIJPEKGW\\2311.html}
}

@online{wuPaletteNeRFPalettebasedColor2022,
  title = {{{PaletteNeRF}}: {{Palette-based Color Editing}} for {{NeRFs}}},
  shorttitle = {{{PaletteNeRF}}},
  author = {Wu, Qiling and Tan, Jianchao and Xu, Kun},
  date = {2022-12-25},
  eprint = {2212.12871},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.12871},
  url = {http://arxiv.org/abs/2212.12871},
  urldate = {2022-12-30},
  abstract = {Neural Radiance Field (NeRF) is a powerful tool to faithfully generate novel views for scenes with only sparse captured images. Despite its strong capability for representing 3D scenes and their appearance, its editing ability is very limited. In this paper, we propose a simple but effective extension of vanilla NeRF, named PaletteNeRF, to enable efficient color editing on NeRF-represented scenes. Motivated by recent palette-based image decomposition works, we approximate each pixel color as a sum of palette colors modulated by additive weights. Instead of predicting pixel colors as in vanilla NeRFs, our method predicts additive weights. The underlying NeRF backbone could also be replaced with more recent NeRF models such as KiloNeRF to achieve real-time editing. Experimental results demonstrate that our method achieves efficient, view-consistent, and artifact-free color editing on a wide range of NeRF-represented scenes.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\8RWYRD53\\Wu 等 - 2022 - PaletteNeRF Palette-based Color Editing for NeRFs.pdf;D\:\\data\\zotero_storage\\storage\\4K9R6VRS\\2212.html}
}

@online{xueGIRAFFEHDHighResolution2022,
  title = {{{GIRAFFE HD}}: {{A High-Resolution 3D-aware Generative Model}}},
  shorttitle = {{{GIRAFFE HD}}},
  author = {Xue, Yang and Li, Yuheng and Singh, Krishna Kumar and Lee, Yong Jae},
  date = {2022-03-28},
  url = {https://arxiv.org/abs/2203.14954v1},
  urldate = {2023-04-27},
  abstract = {3D-aware generative models have shown that the introduction of 3D information can lead to more controllable image generation. In particular, the current state-of-the-art model GIRAFFE can control each object's rotation, translation, scale, and scene camera pose without corresponding supervision. However, GIRAFFE only operates well when the image resolution is low. We propose GIRAFFE HD, a high-resolution 3D-aware generative model that inherits all of GIRAFFE's controllable features while generating high-quality, high-resolution images (\$512\^{}2\$ resolution and above). The key idea is to leverage a style-based neural renderer, and to independently generate the foreground and background to force their disentanglement while imposing consistency constraints to stitch them together to composite a coherent final image. We demonstrate state-of-the-art 3D controllable high-resolution image generation on multiple natural image datasets.},
  langid = {english},
  pubstate = {preprint},
  file = {D:\data\zotero_storage\storage\Q6IRUH8L\Xue 等 - 2022 - GIRAFFE HD A High-Resolution 3D-aware Generative .pdf}
}

@online{xuGridguidedNeuralRadiance2023,
  title = {Grid-Guided {{Neural Radiance Fields}} for {{Large Urban Scenes}}},
  author = {Xu, Linning and Xiangli, Yuanbo and Peng, Sida and Pan, Xingang and Zhao, Nanxuan and Theobalt, Christian and Dai, Bo and Lin, Dahua},
  date = {2023-03-24},
  eprint = {2303.14001},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.14001},
  url = {http://arxiv.org/abs/2303.14001},
  urldate = {2023-04-02},
  abstract = {Purely MLP-based neural radiance fields (NeRF-based methods) often suffer from underfitting with blurred renderings on large-scale scenes due to limited model capacity. Recent approaches propose to geographically divide the scene and adopt multiple sub-NeRFs to model each region individually, leading to linear scale-up in training costs and the number of sub-NeRFs as the scene expands. An alternative solution is to use a feature grid representation, which is computationally efficient and can naturally scale to a large scene with increased grid resolutions. However, the feature grid tends to be less constrained and often reaches suboptimal solutions, producing noisy artifacts in renderings, especially in regions with complex geometry and texture. In this work, we present a new framework that realizes high-fidelity rendering on large urban scenes while being computationally efficient. We propose to use a compact multiresolution ground feature plane representation to coarsely capture the scene, and complement it with positional encoding inputs through another NeRF branch for rendering in a joint learning fashion. We show that such an integration can utilize the advantages of two alternative solutions: a light-weighted NeRF is sufficient, under the guidance of the feature grid representation, to render photorealistic novel views with fine details; and the jointly optimized ground feature planes, can meanwhile gain further refinements, forming a more accurate and compact feature space and output much more natural rendering results.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\BM6FCBHY\\Xu et al. - 2023 - Grid-guided Neural Radiance Fields for Large Urban.pdf;D\:\\data\\zotero_storage\\storage\\E4GYXEWA\\2303.html}
}

@article{xuPointNeRFPointbasedNeurala,
  title = {Point-{{NeRF}}: {{Point-based Neural Radiance Fields}}},
  author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
  langid = {english},
  file = {D:\data\zotero_storage\storage\WR2S28FX\Xu 等 - Point-NeRF Point-based Neural Radiance Fields.pdf}
}

@online{yangS3NeRFNeuralReflectance2022,
  title = {S3-{{NeRF}}: {{Neural Reflectance Field}} from {{Shading}} and {{Shadow}} under a {{Single Viewpoint}}},
  shorttitle = {S\$\^{}3\$-{{NeRF}}},
  author = {Yang, Wenqi and Chen, Guanying and Chen, Chaofeng and Chen, Zhenfang and Wong, Kwan-Yee K.},
  date = {2022-10-17},
  doi = {10.48550/arXiv.2210.08936},
  url = {https://arxiv.org/abs/2210.08936v1},
  urldate = {2023-02-06},
  abstract = {In this paper, we address the "dual problem" of multi-view scene reconstruction in which we utilize single-view images captured under different point lights to learn a neural scene representation. Different from existing single-view methods which can only recover a 2.5D scene representation (i.e., a normal / depth map for the visible surface), our method learns a neural reflectance field to represent the 3D geometry and BRDFs of a scene. Instead of relying on multi-view photo-consistency, our method exploits two information-rich monocular cues, namely shading and shadow, to infer scene geometry. Experiments on multiple challenging datasets show that our method is capable of recovering 3D geometry, including both visible and invisible parts, of a scene from single-view images. Thanks to the neural reflectance field representation, our method is robust to depth discontinuities. It supports applications like novel-view synthesis and relighting. Our code and model can be found at https://ywq.github.io/s3nerf.},
  langid = {english},
  organization = {arXiv.org},
  file = {D\:\\data\\papers\\terrain\\s3nerf.pdf;D\:\\data\\zotero_storage\\storage\\699GINLH\\Yang 等 - 2022 - S$^3$-NeRF Neural Reflectance Field from Shading .pdf}
}

@online{yuPlenoxelsRadianceFields2021,
  title = {Plenoxels: {{Radiance Fields}} without {{Neural Networks}}},
  shorttitle = {Plenoxels},
  author = {Yu, Alex and Fridovich-Keil, Sara and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
  date = {2021-12-09},
  eprint = {2112.05131},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.05131},
  url = {http://arxiv.org/abs/2112.05131},
  urldate = {2023-04-20},
  abstract = {We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\KIVYHJUU\\Yu 等 - 2021 - Plenoxels Radiance Fields without Neural Networks.pdf;D\:\\data\\zotero_storage\\storage\\D8QKIJGC\\2112.html}
}

@online{zhangUnreasonableEffectivenessDeep2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018-04-10},
  eprint = {1801.03924},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.03924},
  url = {http://arxiv.org/abs/1801.03924},
  urldate = {2023-04-12},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\WJNZU9V5\\Zhang 等 - 2018 - The Unreasonable Effectiveness of Deep Features as.pdf;D\:\\data\\zotero_storage\\storage\\V6HNHWC4\\1801.html}
}

@online{zhouNeRFLiXHighQualityNeural2023,
  title = {{{NeRFLiX}}: {{High-Quality Neural View Synthesis}} by {{Learning}} a {{Degradation-Driven Inter-viewpoint MiXer}}},
  shorttitle = {{{NeRFLiX}}},
  author = {Zhou, Kun and Li, Wenbo and Wang, Yi and Hu, Tao and Jiang, Nianjuan and Han, Xiaoguang and Lu, Jiangbo},
  date = {2023-03-22},
  eprint = {2303.06919},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.06919},
  url = {http://arxiv.org/abs/2303.06919},
  urldate = {2023-04-17},
  abstract = {Neural radiance fields (NeRF) show great success in novel view synthesis. However, in real-world scenes, recovering high-quality details from the source images is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality training frames, the synthetic novel views produced by NeRF models still suffer from notable rendering artifacts, such as noise, blur, etc. Towards to improve the synthesis quality of NeRF-based approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by learning a degradation-driven inter-viewpoint mixer. Specially, we design a NeRF-style degradation modeling approach and construct large-scale training data, enabling the possibility of effectively removing NeRF-native rendering artifacts for existing deep neural networks. Moreover, beyond the degradation removal, we propose an inter-viewpoint aggregation framework that is able to fuse highly related high-quality training images, pushing the performance of cutting-edge NeRF models to entirely new levels and producing highly photo-realistic synthetic views.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\RSF9MV3X\\Zhou 等 - 2023 - NeRFLiX High-Quality Neural View Synthesis by Lea.pdf;D\:\\data\\zotero_storage\\storage\\Q3J5I24R\\2303.html}
}
