@online{abdalGaussianShellMaps2023,
  title = {Gaussian {{Shell Maps}} for {{Efficient 3D Human Generation}}},
  author = {Abdal, Rameen and Yifan, Wang and Shi, Zifan and Xu, Yinghao and Po, Ryan and Kuang, Zhengfei and Chen, Qifeng and Yeung, Dit-Yan and Wetzstein, Gordon},
  date = {2023-11-29},
  eprint = {2311.17857},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17857},
  urldate = {2023-12-06},
  abstract = {Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell--based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user-defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high-quality multi-view consistent renderings at a native resolution of \$512 \textbackslash times 512\$ pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\FIBPLKFT\\Abdal 等 - 2023 - Gaussian Shell Maps for Efficient 3D Human Generat.pdf;D\:\\data\\zotero_storage\\storage\\XQFY4XZI\\2311.html}
}

@online{chenGaussianEditorSwiftControllable2023,
  title = {{{GaussianEditor}}: {{Swift}} and {{Controllable 3D Editing}} with {{Gaussian Splatting}}},
  shorttitle = {{{GaussianEditor}}},
  author = {Chen, Yiwen and Chen, Zilong and Zhang, Chi and Wang, Feng and Yang, Xiaofeng and Wang, Yikai and Cai, Zhongang and Yang, Lei and Liu, Huaping and Lin, Guosheng},
  date = {2023-12-01},
  eprint = {2311.14521},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.14521},
  urldate = {2023-12-06},
  abstract = {3D editing plays a crucial role in many areas such as gaming and virtual reality. Traditional 3D editing methods, which rely on representations like meshes and point clouds, often fall short in realistically depicting complex scenes. On the other hand, methods based on implicit 3D representations, like Neural Radiance Field (NeRF), render complex scenes effectively but suffer from slow processing speeds and limited control over specific scene areas. In response to these challenges, our paper presents GaussianEditor, an innovative and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D representation. GaussianEditor enhances precision and control in editing through our proposed Gaussian semantic tracing, which traces the editing target throughout the training process. Additionally, we propose Hierarchical Gaussian splatting (HGS) to achieve stabilized and fine results under stochastic generative guidance from 2D diffusion models. We also develop editing strategies for efficient object removal and integration, a challenging task for existing methods. Our comprehensive experiments demonstrate GaussianEditor's superior control, efficacy, and rapid performance, marking a significant advancement in 3D editing. Project Page: https://buaacyw.github.io/gaussian-editor/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\66GQ289C\\Chen 等 - 2023 - GaussianEditor Swift and Controllable 3D Editing .pdf;D\:\\data\\zotero_storage\\storage\\IU8E5SHS\\2311.html}
}

@online{chungLucidDreamerDomainfreeGeneration2023,
  title = {{{LucidDreamer}}: {{Domain-free Generation}} of {{3D Gaussian Splatting Scenes}}},
  shorttitle = {{{LucidDreamer}}},
  author = {Chung, Jaeyoung and Lee, Suyoung and Nam, Hyeongjin and Lee, Jaerin and Lee, Kyoung Mu},
  date = {2023-11-23},
  eprint = {2311.13384},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.13384},
  url = {http://arxiv.org/abs/2311.13384},
  urldate = {2023-12-08},
  abstract = {With the widespread usage of VR devices and contents, demands for 3D scene generation techniques become more popular. Existing 3D scene generation models, however, limit the target scene to specific domain, primarily due to their training strategies using 3D scan dataset that is far from the real-world. To address such limitation, we propose LucidDreamer, a domain-free scene generation pipeline by fully leveraging the power of existing large-scale diffusion-based generative model. Our LucidDreamer has two alternate steps: Dreaming and Alignment. First, to generate multi-view consistent images from inputs, we set the point cloud as a geometrical guideline for each image generation. Specifically, we project a portion of point cloud to the desired view and provide the projection as a guidance for inpainting using the generative model. The inpainted images are lifted to 3D space with estimated depth maps, composing a new points. Second, to aggregate the new points into the 3D scene, we propose an aligning algorithm which harmoniously integrates the portions of newly generated 3D scenes. The finally obtained 3D scene serves as initial points for optimizing Gaussian splats. LucidDreamer produces Gaussian splats that are highly-detailed compared to the previous 3D scene generation methods, with no constraint on domain of the target scene. Project page: https://luciddreamer-cvlab.github.io/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\QWIGIAH2\\Chung et al. - 2023 - LucidDreamer Domain-free Generation of 3D Gaussia.pdf;D\:\\data\\zotero_storage\\storage\\CHE63RJI\\2311.html}
}

@online{dasNeuralParametricGaussians2023,
  title = {Neural {{Parametric Gaussians}} for {{Monocular Non-Rigid Object Reconstruction}}},
  author = {Das, Devikalyan and Wewer, Christopher and Yunus, Raza and Ilg, Eddy and Lenssen, Jan Eric},
  date = {2023-12-02},
  eprint = {2312.01196},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.01196},
  urldate = {2023-12-06},
  abstract = {Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, high-quality novel views from camera positions that are significantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object's deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects, maintaining 3D consistency across novel views. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging scenarios with few multi-view cues.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\U3JD3LV2\\Das 等 - 2023 - Neural Parametric Gaussians for Monocular Non-Rigi.pdf;D\:\\data\\zotero_storage\\storage\\IRWNR992\\2312.html}
}

@online{fanLightGaussianUnbounded3D2023,
  title = {{{LightGaussian}}: {{Unbounded 3D Gaussian Compression}} with 15x {{Reduction}} and 200+ {{FPS}}},
  shorttitle = {{{LightGaussian}}},
  author = {Fan, Zhiwen and Wang, Kevin and Wen, Kairun and Zhu, Zehao and Xu, Dejia and Wang, Zhangyang},
  date = {2023-12-04},
  eprint = {2311.17245},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.17245},
  url = {http://arxiv.org/abs/2311.17245},
  urldate = {2023-12-06},
  abstract = {Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency. To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses. In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets. Project website: https://lightgaussian.github.io/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\DNGMCCCK\\Fan 等 - 2023 - LightGaussian Unbounded 3D Gaussian Compression w.pdf;D\:\\data\\zotero_storage\\storage\\67B5U9GD\\2311.html}
}

@online{fengGaussianSplashingDynamic2024,
  title = {Gaussian {{Splashing}}: {{Dynamic Fluid Synthesis}} with {{Gaussian Splatting}}},
  shorttitle = {Gaussian {{Splashing}}},
  author = {Feng, Yutao and Feng, Xiang and Shang, Yintong and Jiang, Ying and Yu, Chang and Zong, Zeshun and Shao, Tianjia and Wu, Hongzhi and Zhou, Kun and Jiang, Chenfanfu and Yang, Yin},
  date = {2024-01-27},
  eprint = {2401.15318},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.15318},
  urldate = {2024-03-01},
  abstract = {We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, please visit our project page at \textbackslash url\{https://amysteriouscat.github.io/GaussianSplashing/\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\996ZFB59\\Feng 等 - 2024 - Gaussian Splashing Dynamic Fluid Synthesis with G.pdf;D\:\\data\\zotero_storage\\storage\\YZ74IULH\\2401.html}
}

@online{fuCOLMAPFree3DGaussian2023,
  title = {{{COLMAP-Free 3D Gaussian Splatting}}},
  author = {Fu, Yang and Liu, Sifei and Kulkarni, Amey and Kautz, Jan and Efros, Alexei A. and Wang, Xiaolong},
  date = {2023-12-12},
  eprint = {2312.07504},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.07504},
  url = {http://arxiv.org/abs/2312.07504},
  urldate = {2024-01-29},
  abstract = {While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https://oasisyang.github.io/colmap-free-3dgs},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\CHQMDAXY\\Fu 等 - 2023 - COLMAP-Free 3D Gaussian Splatting.pdf;D\:\\data\\zotero_storage\\storage\\3UB5RAB4\\2312.html}
}

@online{gaoRelightable3DGaussian2023,
  title = {Relightable {{3D Gaussian}}: {{Real-time Point Cloud Relighting}} with {{BRDF Decomposition}} and {{Ray Tracing}}},
  shorttitle = {Relightable {{3D Gaussian}}},
  author = {Gao, Jian and Gu, Chun and Lin, Youtian and Zhu, Hao and Cao, Xun and Zhang, Li and Yao, Yao},
  date = {2023-11-27},
  eprint = {2311.16043},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.16043},
  urldate = {2023-12-08},
  abstract = {We present a novel differentiable point-based rendering framework for material and lighting decomposition from multi-view images, enabling editing, ray-tracing, and real-time relighting of the 3D point cloud. Specifically, a 3D scene is represented as a set of relightable 3D Gaussian points, where each point is additionally associated with a normal direction, BRDF parameters, and incident lights from different directions. To achieve robust lighting estimation, we further divide incident lights of each point into global and local components, as well as view-dependent visibilities. The 3D scene is optimized through the 3D Gaussian Splatting technique while BRDF and lighting are decomposed by physically-based differentiable rendering. Moreover, we introduce an innovative point-based ray-tracing approach based on the bounding volume hierarchy for efficient visibility baking, enabling real-time rendering and relighting of 3D Gaussian points with accurate shadow effects. Extensive experiments demonstrate improved BRDF estimation and novel view rendering results compared to state-of-the-art material estimation approaches. Our framework showcases the potential to revolutionize the mesh-based graphics pipeline with a relightable, traceable, and editable rendering pipeline solely based on point cloud. Project page:https://nju-3dv.github.io/projects/Relightable3DGaussian/.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\74DJJLI8\\Gao et al. - 2023 - Relightable 3D Gaussian Real-time Point Cloud Rel.pdf;D\:\\data\\zotero_storage\\storage\\7DDNRM6N\\2311.html}
}

@online{guedonSuGaRSurfaceAlignedGaussian2023,
  title = {{{SuGaR}}: {{Surface-Aligned Gaussian Splatting}} for {{Efficient 3D Mesh Reconstruction}} and {{High-Quality Mesh Rendering}}},
  shorttitle = {{{SuGaR}}},
  author = {Guédon, Antoine and Lepetit, Vincent},
  date = {2023-12-02},
  eprint = {2311.12775},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.12775},
  url = {http://arxiv.org/abs/2311.12775},
  urldate = {2023-12-06},
  abstract = {We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality. Our project page is the following: https://anttwo.github.io/sugar/},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\CGFESMH4\\Guédon 和 Lepetit - 2023 - SuGaR Surface-Aligned Gaussian Splatting for Effi.pdf;D\:\\data\\zotero_storage\\storage\\NNMWKMTS\\2311.html}
}

@online{huangPointMoveInteractive2023,
  title = {Point'n {{Move}}: {{Interactive Scene Object Manipulation}} on {{Gaussian Splatting Radiance Fields}}},
  shorttitle = {Point'n {{Move}}},
  author = {Huang, Jiajun and Yu, Hongchuan},
  date = {2023-11-28},
  eprint = {2311.16737},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.16737},
  urldate = {2023-12-06},
  abstract = {We propose Point'n Move, a method that achieves interactive scene object manipulation with exposed region inpainting. Interactivity here further comes from intuitive object selection and real-time editing. To achieve this, we adopt Gaussian Splatting Radiance Field as the scene representation and fully leverage its explicit nature and speed advantage. Its explicit representation formulation allows us to devise a 2D prompt points to 3D mask dual-stage self-prompting segmentation algorithm, perform mask refinement and merging, minimize change as well as provide good initialization for scene inpainting and perform editing in real-time without per-editing training, all leads to superior quality and performance. We test our method by performing editing on both forward-facing and 360 scenes. We also compare our method against existing scene object removal methods, showing superior quality despite being more capable and having a speed advantage.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\3NPI7CR9\\Huang 和 Yu - 2023 - Point'n Move Interactive Scene Object Manipulatio.pdf;D\:\\data\\zotero_storage\\storage\\B3JXTLAR\\2311.html}
}

@online{jiangHiFi4GHighFidelityHuman2023,
  title = {{{HiFi4G}}: {{High-Fidelity Human Performance Rendering}} via {{Compact Gaussian Splatting}}},
  shorttitle = {{{HiFi4G}}},
  author = {Jiang, Yuheng and Shen, Zhehao and Wang, Penghao and Su, Zhuo and Hong, Yu and Zhang, Yingliang and Yu, Jingyi and Xu, Lan},
  date = {2023-12-07},
  eprint = {2312.03461},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03461},
  url = {http://arxiv.org/abs/2312.03461},
  urldate = {2024-01-29},
  abstract = {We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\LW2BAMWG\\Jiang 等 - 2023 - HiFi4G High-Fidelity Human Performance Rendering .pdf;D\:\\data\\zotero_storage\\storage\\4NK3P7R9\\2312.html}
}

@online{keethaSplaTAMSplatTrack2023,
  title = {{{SplaTAM}}: {{Splat}}, {{Track}} \& {{Map 3D Gaussians}} for {{Dense RGB-D SLAM}}},
  shorttitle = {{{SplaTAM}}},
  author = {Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan, Deva and Luiten, Jonathon},
  date = {2023-12-04},
  eprint = {2312.02126},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.02126},
  urldate = {2023-12-06},
  abstract = {Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2× state-of-theart performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {D:\data\zotero_storage\storage\RR8SA4UC\Keetha 等 - 2023 - SplaTAM Splat, Track & Map 3D Gaussians for Dense.pdf}
}

@online{kerbl3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
  date = {2023-08-08},
  eprint = {2308.04079},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.04079},
  url = {http://arxiv.org/abs/2308.04079},
  urldate = {2023-08-24},
  abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ({$>$}= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\73AMMNLH\\Kerbl 等 - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field.pdf;D\:\\data\\zotero_storage\\storage\\LPIXKIAE\\2308.html}
}

@online{kocabasHUGSHumanGaussian2023,
  title = {{{HUGS}}: {{Human Gaussian Splats}}},
  shorttitle = {{{HUGS}}},
  author = {Kocabas, Muhammed and Chang, Jen-Hao Rick and Gabriel, James and Tuzel, Oncel and Ranjan, Anurag},
  date = {2023-11-29},
  eprint = {2311.17910},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17910},
  urldate = {2023-12-06},
  abstract = {Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being \textasciitilde 100x faster to train over previous work. Our code will be announced here: https://github.com/apple/ml-hugs},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\58SWCDN8\\Kocabas 等 - 2023 - HUGS Human Gaussian Splats.pdf;D\:\\data\\zotero_storage\\storage\\QYBWVUW4\\2311.html}
}

@online{liangGauFReGaussianDeformation2023,
  title = {{{GauFRe}}: {{Gaussian Deformation Fields}} for {{Real-time Dynamic Novel View Synthesis}}},
  shorttitle = {{{GauFRe}}},
  author = {Liang, Yiqing and Khan, Numair and Li, Zhengqin and Nguyen-Phuoc, Thu and Lanman, Douglas and Tompkin, James and Xiao, Lei},
  date = {2023-12-18},
  eprint = {2312.11458},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.11458},
  urldate = {2024-03-01},
  abstract = {We propose a method for dynamic scene reconstruction using deformable 3D Gaussians that is tailored for monocular video. Building upon the efficiency of Gaussian splatting, our approach extends the representation to accommodate dynamic elements via a deformable set of Gaussians residing in a canonical space, and a time-dependent deformation field defined by a multi-layer perceptron (MLP). Moreover, under the assumption that most natural scenes have large regions that remain static, we allow the MLP to focus its representational power by additionally including a static Gaussian point cloud. The concatenated dynamic and static point clouds form the input for the Gaussian Splatting rasterizer, enabling real-time rendering. The differentiable pipeline is optimized end-to-end with a self-supervised rendering loss. Our method achieves results that are comparable to state-of-the-art dynamic neural radiance field methods while allowing much faster optimization and rendering. Project website: https://lynl7130.github.io/gaufre/index.html},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\YSGQL8R8\\Liang 等 - 2023 - GauFRe Gaussian Deformation Fields for Real-time .pdf;D\:\\data\\zotero_storage\\storage\\MVC52WPW\\2312.html}
}

@online{linGaussianFlow4DReconstruction2023,
  title = {Gaussian-{{Flow}}: {{4D Reconstruction}} with {{Dynamic 3D Gaussian Particle}}},
  shorttitle = {Gaussian-{{Flow}}},
  author = {Lin, Youtian and Dai, Zuozhuo and Zhu, Siyu and Yao, Yao},
  date = {2023-12-06},
  eprint = {2312.03431},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03431},
  url = {http://arxiv.org/abs/2312.03431},
  urldate = {2024-01-29},
  abstract = {We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a \$5\textbackslash times\$ faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality. Project page: https://nju-3dv.github.io/projects/Gaussian-Flow},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\Y8GJE4QY\\Lin 等 - 2023 - Gaussian-Flow 4D Reconstruction with Dynamic 3D G.pdf;D\:\\data\\zotero_storage\\storage\\XV8S9I2M\\2312.html}
}

@online{liuAnimatable3DGaussian2023,
  title = {Animatable {{3D Gaussian}}: {{Fast}} and {{High-Quality Reconstruction}} of {{Multiple Human Avatars}}},
  shorttitle = {Animatable {{3D Gaussian}}},
  author = {Liu, Yang and Huang, Xiang and Qin, Minghan and Lin, Qinwei and Wang, Haoqian},
  date = {2023-11-29},
  eprint = {2311.16482},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.16482},
  urldate = {2023-12-05},
  abstract = {Neural radiance fields are capable of reconstructing high-quality drivable human avatars but are expensive to train and render. To reduce consumption, we propose Animatable 3D Gaussian, which learns human avatars from input images and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of skinned 3D Gaussians and a corresponding skeleton in canonical space and deforming 3D Gaussians to posed space according to the input poses. We introduce hash-encoded shape and appearance to speed up training and propose time-dependent ambient occlusion to achieve high-quality reconstructions in scenes containing complex motions and dynamic shadows. On both novel view synthesis and novel pose synthesis tasks, our method outperforms existing methods in terms of training time, rendering speed, and reconstruction quality. Our method can be easily extended to multi-human scenes and achieve comparable novel view synthesis results on a scene with ten people in only 25 seconds of training.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\PCHTRHFS\\Liu 等 - 2023 - Animatable 3D Gaussian Fast and High-Quality Reco.pdf;D\:\\data\\zotero_storage\\storage\\C2IG4RTB\\2311.html}
}

@online{liuHumanGaussianTextDriven3D2023,
  title = {{{HumanGaussian}}: {{Text-Driven 3D Human Generation}} with {{Gaussian Splatting}}},
  shorttitle = {{{HumanGaussian}}},
  author = {Liu, Xian and Zhan, Xiaohang and Tang, Jiaxiang and Shan, Ying and Zeng, Gang and Lin, Dahua and Liu, Xihui and Liu, Ziwei},
  date = {2023-11-28},
  eprint = {2311.17061},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17061},
  urldate = {2023-12-06},
  abstract = {Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\2TYBPCGB\\Liu 等 - 2023 - HumanGaussian Text-Driven 3D Human Generation wit.pdf;D\:\\data\\zotero_storage\\storage\\CNC5WLNY\\2311.html}
}

@online{luitenDynamic3DGaussians2023,
  title = {Dynamic {{3D Gaussians}}: {{Tracking}} by {{Persistent Dynamic View Synthesis}}},
  shorttitle = {Dynamic {{3D Gaussians}}},
  author = {Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva},
  date = {2023-08-18},
  eprint = {2308.09713},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.09713},
  url = {http://arxiv.org/abs/2308.09713},
  urldate = {2023-09-16},
  abstract = {We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians' motion and rotation with local-rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\T9H9B74V\\Luiten et al. - 2023 - Dynamic 3D Gaussians Tracking by Persistent Dynam.pdf;D\:\\data\\zotero_storage\\storage\\FKWDJT2I\\2308.html}
}

@online{luScaffoldGSStructured3D2023,
  title = {Scaffold-{{GS}}: {{Structured 3D Gaussians}} for {{View-Adaptive Rendering}}},
  shorttitle = {Scaffold-{{GS}}},
  author = {Lu, Tao and Yu, Mulin and Xu, Linning and Xiangli, Yuanbo and Wang, Limin and Lin, Dahua and Dai, Bo},
  date = {2023-11-30},
  eprint = {2312.00109},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.00109},
  urldate = {2023-12-06},
  abstract = {Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications. The recent 3D Gaussian Splatting method has achieved the state-of-the-art rendering quality and speed combining the benefits of both primitive-based representations and volumetric representations. However, it often leads to heavily redundant Gaussians that try to fit every training view, neglecting the underlying scene geometry. Consequently, the resulting model becomes less robust to significant view changes, texture-less area and lighting effects. We introduce Scaffold-GS, which uses anchor points to distribute local 3D Gaussians, and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. Anchor growing and pruning strategies are developed based on the importance of neural Gaussians to reliably improve the scene coverage. We show that our method effectively reduces redundant Gaussians while delivering high-quality rendering. We also demonstrates an enhanced capability to accommodate scenes with varying levels-of-detail and view-dependent observations, without sacrificing the rendering speed.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\8BYZ9R97\\Lu 等 - 2023 - Scaffold-GS Structured 3D Gaussians for View-Adap.pdf;D\:\\data\\zotero_storage\\storage\\LPFYQAGS\\2312.html}
}

@online{pokhariyaMANUSMarkerlessHandObject2023,
  title = {{{MANUS}}: {{Markerless Hand-Object Grasp Capture}} Using {{Articulated 3D Gaussians}}},
  shorttitle = {{{MANUS}}},
  author = {Pokhariya, Chandradeep and Shah, Ishaan N. and Xing, Angela and Li, Zekun and Chen, Kefan and Sharma, Avinash and Sridhar, Srinath},
  date = {2023-12-04},
  eprint = {2312.02137},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.02137},
  urldate = {2023-12-06},
  abstract = {Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality. However, this challenging problem requires accurate modeling of the contact between hands and objects. To capture grasps, existing methods use skeletons, meshes, or parametric models that can cause misalignments resulting in inaccurate contacts. We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians. We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands. Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object. For the most accurate results, our method requires tens of camera views that current datasets do not provide. We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3 subjects, and comprising over 7M frames. In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\WCIK37RU\\Pokhariya 等 - 2023 - MANUS Markerless Hand-Object Grasp Capture using .pdf;D\:\\data\\zotero_storage\\storage\\2T4MCGFY\\2312.html}
}

@online{qianGaussianAvatarsPhotorealisticHead2023,
  title = {{{GaussianAvatars}}: {{Photorealistic Head Avatars}} with {{Rigged 3D Gaussians}}},
  shorttitle = {{{GaussianAvatars}}},
  author = {Qian, Shenhan and Kirschstein, Tobias and Schoneveld, Liam and Davoli, Davide and Giebenhain, Simon and Nießner, Matthias},
  date = {2023-12-04},
  eprint = {2312.02069},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.02069},
  urldate = {2023-12-06},
  abstract = {We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\NXC8GX4D\\Qian 等 - 2023 - GaussianAvatars Photorealistic Head Avatars with .pdf;D\:\\data\\zotero_storage\\storage\\5TUZKC5S\\2312.html}
}

@online{saitoRelightableGaussianCodec2023,
  title = {Relightable {{Gaussian Codec Avatars}}},
  author = {Saito, Shunsuke and Schwartz, Gabriel and Simon, Tomas and Li, Junxuan and Nam, Giljoo},
  date = {2023-12-06},
  eprint = {2312.03704},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03704},
  url = {http://arxiv.org/abs/2312.03704},
  urldate = {2024-01-29},
  abstract = {The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with spatially all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {D\:\\data\\zotero_storage\\storage\\YFQRJA88\\Saito 等 - 2023 - Relightable Gaussian Codec Avatars.pdf;D\:\\data\\zotero_storage\\storage\\CHENDKVK\\2312.html}
}

@online{tangDreamGaussianGenerativeGaussian2023,
  title = {{{DreamGaussian}}: {{Generative Gaussian Splatting}} for {{Efficient 3D Content Creation}}},
  shorttitle = {{{DreamGaussian}}},
  author = {Tang, Jiaxiang and Ren, Jiawei and Zhou, Hang and Liu, Ziwei and Zeng, Gang},
  date = {2023-09-28},
  eprint = {2309.16653},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16653},
  url = {http://arxiv.org/abs/2309.16653},
  urldate = {2023-10-17},
  abstract = {Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\G5IG4VP2\\Tang 等 - 2023 - DreamGaussian Generative Gaussian Splatting for E.pdf;D\:\\data\\zotero_storage\\storage\\56YGEUYT\\2309.html}
}

@online{vilesovCG3DCompositionalGeneration2023,
  title = {{{CG3D}}: {{Compositional Generation}} for {{Text-to-3D}} via {{Gaussian Splatting}}},
  shorttitle = {{{CG3D}}},
  author = {Vilesov, Alexander and Chari, Pradyumna and Kadambi, Achuta},
  date = {2023-11-29},
  eprint = {2311.17907},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17907},
  urldate = {2023-12-06},
  abstract = {With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration. Recently, these models have been shown to provide useful guidance for the generation of 3D graphics assets. However, existing work in text-conditioned 3D generation faces fundamental constraints: (i) inability to generate detailed, multi-object scenes, (ii) inability to textually control multi-object configurations, and (iii) physically realistic scene composition. In this work, we propose CG3D, a method for compositionally generating scalable 3D assets that resolves these constraints. We find that explicit Gaussian radiance fields, parameterized to allow for compositions of objects, possess the capability to enable semantically and physically consistent scenes. By utilizing a guidance framework built around this explicit representation, we show state of the art results, capable of even exceeding the guiding diffusion model in terms of object combinations and physics accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\MQYLGBST\\Vilesov 等 - 2023 - CG3D Compositional Generation for Text-to-3D via .pdf;D\:\\data\\zotero_storage\\storage\\5Z2JRIY8\\2311.html}
}

@online{wangGaussianHeadImpressive3D2023,
  title = {{{GaussianHead}}: {{Impressive 3D Gaussian-based Head Avatars}} with {{Dynamic Hybrid Neural Field}}},
  shorttitle = {{{GaussianHead}}},
  author = {Wang, Jie and Li, Xianyan and Xie, Jiucheng and Xu, Feng and Gao, Hao},
  date = {2023-12-04},
  eprint = {2312.01632},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.01632},
  urldate = {2023-12-06},
  abstract = {Previous head avatar methods have mostly relied on fixed explicit primitives (mesh, point) or implicit surfaces (Sign Distance Function) and volumetric neural radiance field, it challenging to strike a balance among high fidelity, training speed, and resource consumption. The recent popularity of hybrid field has brought novel representation, but is limited by relying on parameterization factors obtained through fixed mappings. We propose GaussianHead: an head avatar algorithm based on anisotropic 3D gaussian primitives. We leverage canonical gaussians to represent dynamic scenes. Using explicit "dynamic" tri-plane as an efficient container for parameterized head geometry, aligned well with factors in the underlying geometry and tri-plane, we obtain aligned canonical factors for the canonical gaussians. With a tiny MLP, factors are decoded into opacity and spherical harmonic coefficients of 3D gaussian primitives. Finally, we use efficient differentiable gaussian rasterizer for rendering. Our approach benefits significantly from our novel representation based on 3D gaussians, and the proper alignment transformation of underlying geometry structures and factors in tri-plane eliminates biases introduced by fixed mappings. Compared to state-of-the-art techniques, we achieve optimal visual results in tasks such as self-reconstruction, novel view synthesis, and cross-identity reenactment while maintaining high rendering efficiency (0.12s per frame). Even the pores around the nose are clearly visible in some cases. Code and additional video can be found on the project homepage.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\5LSSH3SP\\Wang 等 - 2023 - GaussianHead Impressive 3D Gaussian-based Head Av.pdf;D\:\\data\\zotero_storage\\storage\\URWNE5KT\\2312.html}
}

@online{xiePhysGaussianPhysicsIntegrated3D2023,
  title = {{{PhysGaussian}}: {{Physics-Integrated 3D Gaussians}} for {{Generative Dynamics}}},
  shorttitle = {{{PhysGaussian}}},
  author = {Xie, Tianyi and Zong, Zeshun and Qiu, Yuxing and Li, Xuan and Feng, Yutao and Yang, Yin and Jiang, Chenfanfu},
  date = {2023-11-22},
  eprint = {2311.12198},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.12198},
  url = {http://arxiv.org/abs/2311.12198},
  urldate = {2023-12-06},
  abstract = {We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel motion synthesis. Employing a custom Material Point Method (MPM), our approach enriches 3D Gaussian kernels with physically meaningful kinematic deformation and mechanical stress attributes, all evolved in line with continuum mechanics principles. A defining characteristic of our method is the seamless integration between physical simulation and visual rendering: both components utilize the same 3D Gaussian kernels as their discrete representations. This negates the necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or any other geometry embedding, highlighting the principle of "what you see is what you simulate (WS\$\^{}2\$)." Our method demonstrates exceptional versatility across a wide variety of materials--including elastic entities, metals, non-Newtonian fluids, and granular materials--showcasing its strong capabilities in creating diverse visual content with novel viewpoints and movements. Our project page is at: https://xpandora.github.io/PhysGaussian/},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\HW7UIQLA\\Xie 等 - 2023 - PhysGaussian Physics-Integrated 3D Gaussians for .pdf;D\:\\data\\zotero_storage\\storage\\P9AYEQW4\\2311.html}
}

@online{xiongSparseGSRealTime3602023,
  title = {{{SparseGS}}: {{Real-Time}} 360\{\textbackslash deg\} {{Sparse View Synthesis}} Using {{Gaussian Splatting}}},
  shorttitle = {{{SparseGS}}},
  author = {Xiong, Haolin and Muttukuru, Sairisheek and Upadhyay, Rishi and Chari, Pradyumna and Kadambi, Achuta},
  date = {2023-11-30},
  eprint = {2312.00206},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2312.00206},
  urldate = {2024-03-12},
  abstract = {The problem of novel view synthesis has grown significantly in popularity recently with the introduction of Neural Radiance Fields (NeRFs) and other implicit scene representation methods. A recent advance, 3D Gaussian Splatting (3DGS), leverages an explicit representation to achieve real-time rendering with high-quality results. However, 3DGS still requires an abundance of training views to generate a coherent scene representation. In few shot settings, similar to NeRF, 3DGS tends to overfit to training views, causing background collapse and excessive floaters, especially as the number of training views are reduced. We propose a method to enable training coherent 3DGS-based radiance fields of 360 scenes from sparse training views. We find that using naive depth priors is not sufficient and integrate depth priors with generative and explicit constraints to reduce background collapse, remove floaters, and enhance consistency from unseen viewpoints. Experiments show that our method outperforms base 3DGS by up to 30.5\% and NeRF-based methods by up to 15.6\% in LPIPS on the MipNeRF-360 dataset with substantially less training and inference cost.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {D\:\\data\\zotero_storage\\storage\\VLBEUILM\\Xiong 等 - 2023 - SparseGS Real-Time 360 deg Sparse View Synthesi.pdf;D\:\\data\\zotero_storage\\storage\\J68YDUWX\\2312.html}
}

@online{yanMultiScale3DGaussian2023,
  title = {Multi-{{Scale 3D Gaussian Splatting}} for {{Anti-Aliased Rendering}}},
  author = {Yan, Zhiwen and Low, Weng Fei and Chen, Yu and Lee, Gim Hee},
  date = {2023-11-27},
  eprint = {2311.17089},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17089},
  urldate = {2023-12-06},
  abstract = {3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13\textbackslash\%-66\textbackslash\% PSNR and 160\textbackslash\%-2400\textbackslash\% rendering speed improvement at 4\$\textbackslash times\$-128\$\textbackslash times\$ scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splatting.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\YGUNY48D\\Yan 等 - 2023 - Multi-Scale 3D Gaussian Splatting for Anti-Aliased.pdf;D\:\\data\\zotero_storage\\storage\\44CNNHG6\\2311.html}
}

@online{yeGaussianGroupingSegment2023,
  title = {Gaussian {{Grouping}}: {{Segment}} and {{Edit Anything}} in {{3D Scenes}}},
  shorttitle = {Gaussian {{Grouping}}},
  author = {Ye, Mingqiao and Danelljan, Martin and Yu, Fisher and Ke, Lei},
  date = {2023-12-01},
  eprint = {2312.00732},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.00732},
  urldate = {2023-12-06},
  abstract = {The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by SAM, along with introduced 3D spatial consistency regularization. Comparing to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization and scene recomposition. Our code and models will be at https://github.com/lkeab/gaussian-grouping.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\SD2LS886\\Ye 等 - 2023 - Gaussian Grouping Segment and Edit Anything in 3D.pdf;D\:\\data\\zotero_storage\\storage\\KEPUTZ5F\\2312.html}
}

@online{yeMathematicalSupplementTexttt2023,
  title = {Mathematical {{Supplement}} for the \$\textbackslash texttt\{gsplat\}\$ {{Library}}},
  author = {Ye, Vickie and Kanazawa, Angjoo},
  date = {2023-12-04},
  eprint = {2312.02121},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2312.02121},
  urldate = {2023-12-06},
  abstract = {This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization at github.com/nerfstudio-project/gsplat .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Mathematical Software,Mathematics - Numerical Analysis},
  file = {D\:\\data\\zotero_storage\\storage\\NVHTGNS8\\Ye 和 Kanazawa - 2023 - Mathematical Supplement for the $texttt gsplat $ .pdf;D\:\\data\\zotero_storage\\storage\\QBUHD3Y6\\2312.html}
}

@online{zhengGPSGaussianGeneralizablePixelwise2023,
  title = {{{GPS-Gaussian}}: {{Generalizable Pixel-wise 3D Gaussian Splatting}} for {{Real-time Human Novel View Synthesis}}},
  shorttitle = {{{GPS-Gaussian}}},
  author = {Zheng, Shunyuan and Zhou, Boyao and Shao, Ruizhi and Liu, Boning and Zhang, Shengping and Nie, Liqiang and Liu, Yebin},
  date = {2023-12-04},
  eprint = {2312.02155},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.02155},
  urldate = {2023-12-06},
  abstract = {We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\QLCKEGLR\\Zheng 等 - 2023 - GPS-Gaussian Generalizable Pixel-wise 3D Gaussian.pdf;D\:\\data\\zotero_storage\\storage\\954859A3\\2312.html}
}

@online{zhuFSGSRealTimeFewshot2023,
  title = {{{FSGS}}: {{Real-Time Few-shot View Synthesis}} Using {{Gaussian Splatting}}},
  shorttitle = {{{FSGS}}},
  author = {Zhu, Zehao and Fan, Zhiwen and Jiang, Yifan and Wang, Zhangyang},
  date = {2023-12-01},
  eprint = {2312.00451},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.00451},
  urldate = {2023-12-06},
  abstract = {Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\data\\zotero_storage\\storage\\2W433MGT\\Zhu 等 - 2023 - FSGS Real-Time Few-shot View Synthesis using Gaus.pdf;D\:\\data\\zotero_storage\\storage\\SUFAR3DM\\2312.html}
}
