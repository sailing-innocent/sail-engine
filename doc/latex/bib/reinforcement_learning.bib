@article{bardHanabiChallengeNew2020,
  title = {The {{Hanabi Challenge}}: {{A New Frontier}} for {{AI Research}}},
  shorttitle = {The {{Hanabi Challenge}}},
  author = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},
  date = {2020-03},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {280},
  eprint = {1902.00506},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {103216},
  issn = {00043702},
  doi = {10.1016/j.artint.2019.103216},
  url = {http://arxiv.org/abs/1902.00506},
  urldate = {2023-06-07},
  abstract = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\HMCWQ96G\\Bard et al. - 2020 - The Hanabi Challenge A New Frontier for AI Resear.pdf;D\:\\data\\zotero_storage\\storage\\MFFC8VSF\\1902.html}
}

@online{cobbePhasicPolicyGradient2020,
  title = {Phasic {{Policy Gradient}}},
  author = {Cobbe, Karl and Hilton, Jacob and Klimov, Oleg and Schulman, John},
  date = {2020-09-09},
  eprint = {2009.04416},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.04416},
  urldate = {2023-10-17},
  abstract = {We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework which modifies traditional on-policy actor-critic methods by separating policy and value function training into distinct phases. In prior methods, one must choose between using a shared network or separate networks to represent the policy and value function. Using separate networks avoids interference between objectives, while using a shared network allows useful features to be shared. PPG is able to achieve the best of both worlds by splitting optimization into two phases, one that advances training and one that distills features. PPG also enables the value function to be more aggressively optimized with a higher level of sample reuse. Compared to PPO, we find that PPG significantly improves sample efficiency on the challenging Procgen Benchmark.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\ZHRG7QEH\\Cobbe 等 - 2020 - Phasic Policy Gradient.pdf;D\:\\data\\zotero_storage\\storage\\9VY6LITN\\2009.html}
}

@online{ilyasCloserLookDeep2020,
  title = {A {{Closer Look}} at {{Deep Policy Gradients}}},
  author = {Ilyas, Andrew and Engstrom, Logan and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  date = {2020-05-25},
  eprint = {1811.02553},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.02553},
  urldate = {2023-10-17},
  abstract = {We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the "true" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.},
  pubstate = {preprint},
  version = {4},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Statistics - Machine Learning},
  file = {D\:\\data\\zotero_storage\\storage\\8VNKZ85P\\Ilyas 等 - 2020 - A Closer Look at Deep Policy Gradients.pdf;D\:\\data\\zotero_storage\\storage\\22YKSQIG\\1811.html}
}

@online{nekoeiContinuousCoordinationRealistic2021,
  title = {Continuous {{Coordination As}} a {{Realistic Scenario}} for {{Lifelong Learning}}},
  author = {Nekoei, Hadi and Badrinaaraayanan, Akilesh and Courville, Aaron and Chandar, Sarath},
  date = {2021-06-14},
  eprint = {2103.03216},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.03216},
  url = {http://arxiv.org/abs/2103.03216},
  urldate = {2023-06-07},
  abstract = {Current deep reinforcement learning (RL) algorithms are still highly task-specific and lack the ability to generalize to new environments. Lifelong learning (LLL), however, aims at solving multiple tasks sequentially by efficiently transferring and using knowledge between tasks. Despite a surge of interest in lifelong RL in recent years, the lack of a realistic testbed makes robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the other hand, can be seen as a natural scenario for lifelong RL due to its inherent non-stationarity, since the agents' policies change over time. In this work, we introduce a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings. Our setup is based on Hanabi -- a partially-observable, fully cooperative multi-agent game that has been shown to be challenging for zero-shot coordination. Its large strategy space makes it a desirable environment for lifelong RL tasks. We evaluate several recent MARL methods, and benchmark state-of-the-art LLL algorithms in limited memory and computation regimes to shed light on their strengths and weaknesses. This continual learning paradigm also provides us with a pragmatic way of going beyond centralized training which is the most commonly used training protocol in MARL. We empirically show that the agents trained in our setup are able to coordinate well with unseen agents, without any additional assumptions made by previous works. The code and all pre-trained models are available at https://github.com/chandar-lab/Lifelong-Hanabi.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {D:\data\zotero_storage\storage\VEICNFWZ\2103.html}
}

@incollection{wawrzynskiCatLikeRobotRealTime2009,
  title = {A {{Cat-Like Robot Real-Time Learning}} to {{Run}}},
  booktitle = {Adaptive and {{Natural Computing Algorithms}}},
  author = {Wawrzyński, Paweł},
  editor = {Kolehmainen, Mikko and Toivanen, Pekka and Beliczynski, Bartlomiej},
  date = {2009},
  volume = {5495},
  pages = {380--390},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-04921-7_39},
  url = {http://link.springer.com/10.1007/978-3-642-04921-7_39},
  urldate = {2023-06-05},
  abstract = {Actor-Critics constitute an important class of reinforcement learning algorithms that can deal with continuous actions and states in an easy and natural way. In their original, sequential form, these algorithms are usually to slow to be applicable to real-life problems. However, they can be augmented by the technique of experience replay to obtain a satisfying speed of learning without degrading their convergence properties. In this paper experimental results are presented that show that the combination of experience replay and Actor-Critics yields very fast learning algorithms that achieve successful policies for nontrivial control tasks in considerably short time. Namely, a policy for a model of 6degree-of-freedom walking robot is obtained after 4 hours of the robot’s time.},
  isbn = {978-3-642-04920-0 978-3-642-04921-7},
  langid = {english}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  url = {https://doi.org/10.1007/BF00992696},
  urldate = {2023-10-17},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  keywords = {connectionist networks,gradient descent,mathematical analysis,Reinforcement learning},
  file = {D:\data\zotero_storage\storage\29GKF7X8\Williams - 1992 - Simple statistical gradient-following algorithms f.pdf}
}
